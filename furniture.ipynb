{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16e6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34092cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolo11s-cls.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1572a9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.140 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.139 ðŸš€ Python-3.13.2 torch-2.7.0+cu128 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 7817MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=furniture_cls_yolo, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11s-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/classify/train4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=16, workspace=None\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /media/abhi/Abhi/Code/Python/Kaggle/appian/round 2/furniture_cls_yolo/train... found 12000 images in 5 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /media/abhi/Abhi/Code/Python/Kaggle/appian/round 2/furniture_cls_yolo/val... found 1500 images in 5 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /media/abhi/Abhi/Code/Python/Kaggle/appian/round 2/furniture_cls_yolo/test... found 1500 images in 5 classes âœ… \n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 10                  -1  1    664325  ultralytics.nn.modules.head.Classify         [512, 5]                      \n",
      "YOLO11s-cls summary: 86 layers, 5,449,413 parameters, 5,449,413 gradients\n",
      "Transferred 234/236 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 338.5Â±166.0 MB/s, size: 4.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /media/abhi/Abhi/Code/Python/Kaggle/appian/round 2/furniture_cls_yolo/train... 12000 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12000/12000 [00:00<00:00, 12159.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /media/abhi/Abhi/Code/Python/Kaggle/appian/round 2/furniture_cls_yolo/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 423.6Â±206.5 MB/s, size: 6.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /media/abhi/Abhi/Code/Python/Kaggle/appian/round 2/furniture_cls_yolo/val... 1500 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:00<00:00, 11952.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /media/abhi/Abhi/Code/Python/Kaggle/appian/round 2/furniture_cls_yolo/val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 39 weight(decay=0.0), 40 weight(decay=0.0005), 40 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 16 dataloader workers\n",
      "Logging results to \u001b[1mruns/classify/train4\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5     0.713G     0.3175         32        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:28<00:00, 13.36it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 44.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5     0.898G     0.1212         32        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:25<00:00, 14.77it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 52.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.995          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5     0.916G    0.08708         32        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:26<00:00, 14.38it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 54.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.997          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        4/5     0.932G    0.04648         32        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:25<00:00, 14.51it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 53.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5     0.949G    0.01713         32        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 375/375 [00:26<00:00, 14.38it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 53.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.037 hours.\n",
      "Optimizer stripped from runs/classify/train4/weights/last.pt, 11.0MB\n",
      "Optimizer stripped from runs/classify/train4/weights/best.pt, 11.0MB\n",
      "\n",
      "Validating runs/classify/train4/weights/best.pt...\n",
      "Ultralytics 8.3.139 ðŸš€ Python-3.13.2 torch-2.7.0+cu128 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 7817MiB)\n",
      "YOLO11s-cls summary (fused): 47 layers, 5,440,533 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /media/abhi/Abhi/Code/Python/Kaggle/appian/round 2/furniture_cls_yolo/train... found 12000 images in 5 classes âœ… \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /media/abhi/Abhi/Code/Python/Kaggle/appian/round 2/furniture_cls_yolo/val... found 1500 images in 5 classes âœ… \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /media/abhi/Abhi/Code/Python/Kaggle/appian/round 2/furniture_cls_yolo/test... found 1500 images in 5 classes âœ… \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:01<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          1\n",
      "Speed: 0.1ms preprocess, 0.3ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/classify/train4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.train(data='furniture_cls_yolo',epochs = 5,workers=16,batch=32,device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dad07d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ClassifyMetrics.summary of ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
       "\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x74ecccd469f0>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 1.0\n",
       "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
       "results_dict: {'metrics/accuracy_top1': 1.0, 'metrics/accuracy_top5': 1.0, 'fitness': 1.0}\n",
       "save_dir: PosixPath('runs/classify/train2')\n",
       "speed: {'preprocess': 0.05957666116713275, 'inference': 0.20541881699925096, 'loss': 7.552199955777421e-05, 'postprocess': 0.0002088563326954803}\n",
       "task: 'classify'\n",
       "top1: 1.0\n",
       "top5: 1.0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27b71f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /media/abhi/Abhi/Code/Python/Kaggle/appian/round 2/room.jpg: 224x224 table 0.99, chair 0.01, almirah 0.00, fridge 0.00, tv 0.00, 2.0ms\n",
      "Speed: 6.3ms preprocess, 2.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Predicted Class: table\n",
      "Confidence: 0.99\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load your trained classification model\n",
    "model = YOLO('runs/classify/train2/weights/best.pt')  # update path if needed\n",
    "\n",
    "results = model('room.jpg')  # Replace with your actual image path\n",
    "\n",
    "# Display results\n",
    "for r in results:\n",
    "    print(f\"Predicted Class: {r.names[r.probs.top1]}\")\n",
    "    print(f\"Confidence: {r.probs.data[r.probs.top1].item():.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c28f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'almirah', 1: 'chair', 2: 'fridge', 3: 'table', 4: 'tv'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f884f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['almirah', 'chair', 'fridge', 'table', 'tv']\n",
      "YOLO classification dataset ready at: furniture_cls_yolo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_yolo_cls_dataset(base_dir, output_dir=\"furniture_cls_yolo\", test_size=0.2):\n",
    "    # Get class folders\n",
    "    class_folders = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]\n",
    "    class_folders.sort()\n",
    "\n",
    "    # Clean class names\n",
    "    class_name_map = {\n",
    "        f: f.replace(\"_dataset\", \"\").replace(\" dataset\", \"\").replace(\" \", \"_\").lower()\n",
    "        for f in class_folders\n",
    "    }\n",
    "\n",
    "    print(\"Classes:\", list(class_name_map.values()))\n",
    "\n",
    "    # Create target folders\n",
    "    for split in ['train', 'val']:\n",
    "        for class_name in class_name_map.values():\n",
    "            os.makedirs(os.path.join(output_dir, split, class_name), exist_ok=True)\n",
    "\n",
    "    # Move and split\n",
    "    for folder in class_folders:\n",
    "        class_name = class_name_map[folder]\n",
    "        image_paths = glob(os.path.join(base_dir, folder, \"*\"))\n",
    "\n",
    "        train_imgs, val_imgs = train_test_split(image_paths, test_size=test_size, random_state=42)\n",
    "\n",
    "        for split, imgs in zip(['train', 'val'], [train_imgs, val_imgs]):\n",
    "            for i, img_path in enumerate(imgs):\n",
    "                orig_filename = os.path.basename(img_path)\n",
    "                ext = os.path.splitext(orig_filename)[-1]\n",
    "                new_filename = f\"{class_name}_{split}_{i}{ext}\"\n",
    "                dest = os.path.join(output_dir, split, class_name, new_filename)\n",
    "                shutil.copyfile(img_path, dest)\n",
    "\n",
    "    print(f\"YOLO classification dataset ready at: {output_dir}\")\n",
    "\n",
    "# Usage\n",
    "create_yolo_cls_dataset(\"furniture_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dd40ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['almirah', 'chair', 'fridge', 'table', 'tv']\n",
      "YOLO classification dataset ready at: furniture_cls_yolo\n",
      "Splits: {'train': 12000, 'val': 1500, 'test': 1500}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "import torch\n",
    "\n",
    "def create_yolo_cls_dataset(base_dir, output_dir=\"furniture_cls_yolo\", val_size=0.1, test_size=0.1):\n",
    "    # Get class folders\n",
    "    class_folders = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]\n",
    "    class_folders.sort()\n",
    "\n",
    "    # Clean class names\n",
    "    class_name_map = {\n",
    "        f: f.replace(\"_dataset\", \"\").replace(\" dataset\", \"\").replace(\" \", \"_\").lower()\n",
    "        for f in class_folders\n",
    "    }\n",
    "\n",
    "    print(\"Classes:\", list(class_name_map.values()))\n",
    "\n",
    "    # Create target folders\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for class_name in class_name_map.values():\n",
    "            os.makedirs(os.path.join(output_dir, split, class_name), exist_ok=True)\n",
    "\n",
    "    split_paths = {'train': [], 'val': [], 'test': []}\n",
    "\n",
    "    for folder in class_folders:\n",
    "        class_name = class_name_map[folder]\n",
    "        image_paths = glob(os.path.join(base_dir, folder, \"*\"))\n",
    "        num_imgs = len(image_paths)\n",
    "\n",
    "        indices = torch.randperm(num_imgs)  # shuffled indices\n",
    "\n",
    "        test_count = int(num_imgs * test_size)\n",
    "        val_count = int(num_imgs * val_size)\n",
    "        train_count = num_imgs - val_count - test_count\n",
    "\n",
    "        train_idx = indices[:train_count]\n",
    "        val_idx = indices[train_count:train_count+val_count]\n",
    "        test_idx = indices[train_count+val_count:]\n",
    "\n",
    "        splits = {\n",
    "            'train': train_idx,\n",
    "            'val': val_idx,\n",
    "            'test': test_idx,\n",
    "        }\n",
    "\n",
    "        for split, idxs in splits.items():\n",
    "            for i, idx in enumerate(idxs):\n",
    "                img_path = image_paths[idx]\n",
    "                orig_filename = os.path.basename(img_path)\n",
    "                ext = os.path.splitext(orig_filename)[-1]\n",
    "                new_filename = f\"{class_name}_{split}_{i}{ext}\"\n",
    "                dest_path = os.path.join(output_dir, split, class_name, new_filename)\n",
    "                shutil.copyfile(img_path, dest_path)\n",
    "                split_paths[split].append(os.path.relpath(dest_path, output_dir))\n",
    "\n",
    "    # Write split files\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        with open(os.path.join(output_dir, f\"{split}.txt\"), 'w') as f:\n",
    "            for path in split_paths[split]:\n",
    "                f.write(path + \"\\n\")\n",
    "\n",
    "    print(f\"YOLO classification dataset ready at: {output_dir}\")\n",
    "    print(\"Splits:\", {k: len(v) for k, v in split_paths.items()})\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "create_yolo_cls_dataset(\"furniture_dataset\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
